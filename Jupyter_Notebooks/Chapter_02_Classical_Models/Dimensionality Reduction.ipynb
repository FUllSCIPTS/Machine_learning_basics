{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cee9c79-ae63-4675-87df-255510403af3",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" align=\"center\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <font face=\"IranNastaliq\" size=5>\n",
    "      به نام خدا\n",
    "    </font>\n",
    "    <br>\n",
    "    <font size=3>\n",
    "      دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "    </font>\n",
    "    <br>\n",
    "    <font color=blue size=5>\n",
    "      مقدمه‌ای بر یادگیری ماشین\n",
    "    </font>\n",
    "    <br>\n",
    "    <hr/>\n",
    "    <font color=red size=6>\n",
    "      فصل دوم:  کاهش بعد داده‌ها\n",
    "    </font>\n",
    "    <br>\n",
    "      نویسنده:‌ سینا مظاهری\n",
    "    <hr>\n",
    "<br>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba14717-4269-4af7-9901-62b59bd8a751",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: 5; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">1.مقدمه</span>\n",
    "    <br>\n",
    "    در مباحثی از یادگیری نظارت نشده که بیشتر بر روی کاهش بعد داده‌ها تمرکز دارند؛ یک هدف اصلی وجود دارد:  <b> پیدا کردن متغیر‌های تصادفی نهان پیوسته در ساختار مجموعه دادگان داده شده </b>.<br>\n",
    "    این عمل درست بر‌خلاف خوشه‌بندی می‌باشد که در آن ما به دنبال متغیر‌های نهان گسسته می‌باشیم. (پیدا کردن تعداد خوشه‌ها، بررسی رفتار خوشه‌ها، ...)<br>\n",
    "    روش‌های متعددی به منظور کاهش بعد داده‌ها وجود دارند. در این بخش به یکی از پرکاربردترین و معروف‌ترین آن‌ها می پردازیم.<br>\n",
    "    کاهش بعد‌ داده‌ها عموما به سه منظور صورت می‌گیرد:\n",
    "    <ul>\n",
    "        <li>نمایش داده‌ها یا همان Data Visualization برای مجموعه دادگانی که معمولا ابعاد بسیار بالایی دارند یعنی تعداد ابعاد از تعداد داده‌ها بسیار بسیار بیشتر است $(D \\gg N)$</li>\n",
    "        <li>در بعضی از روش‌های یادگیری ماشین نظارت شده که هدف به دست آوردن یک مقدار پیوسته یا گسسته است، کاهش بعد داده‌های ورودی، تاثیر بسزایی برروی عملکرد و نتایج خروجی آن می گذارد.</li> \n",
    "        <li>به منظور فشرده‌سازی داده‌ها استفاده می‌گردد.</li>\n",
    "    </ul>\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25574fb-0e37-45aa-99b8-db7fe7ff82e4",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"><table><tr>\n",
    "<td> <img src=\"https://www.sqlservercentral.com/wp-content/uploads/legacy/c2362193dd68a81c6774bb9668f1178fc2cf4ad4/32947.jpg\" width=\"500\" style=\"vertical-align:middle\">\n",
    " <figcaption><center>شکل1. فشرده‌سازی تصویر</center></figcaption>\n",
    " </td>\n",
    "<td> <img src=\"https://www.researchgate.net/profile/Diego-Peluffo/publication/325363944/figure/fig1/AS:631693463539712@1527618866278/Dimensionality-reduction-effect-over-an-3D-artificial-Swiss-roll-manifold-the-2D.png\" width=\"500\" style=\"vertical-align:middle\">\n",
    " <figcaption><center>شکل2. کاهش بعد داده‌های سه بعدی به دو بعدی</center></figcaption> </td>\n",
    "</tr></table>\n",
    "    </div>\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc8de2-1826-41ac-a8a4-ba878c473e49",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">2. تحلیل مولفه‌های اصلی یا PCA</span>\n",
    "    <br>\n",
    "    یکی از پرکاربردترین روش‌هایی که در کاهش بعد داده‌های یک مجموعه دادگان استفاده می‌شود\n",
    "    <b>تحلیل مولفه‌های اصلی </b>\n",
    "    می‌باشد. این روش از دو منظر قابل تعریف است که هر دو معادل یکدیگر می‌باشند:\n",
    "    <ul>\n",
    "  <li>نگاشت متعامد مجموعه دادگان به یک فضای برداری خطی با ابعاد کمتر از فضای برداری اصلی خود دادگان به گونه‌ای که واریانس دادگان تبدیل یافته بیشینه شود. به این زیرفضای کاهش یافته،\n",
    "      <b>زیرفضای اصلی (principal subspace)</b>\n",
    "      نیز گفته می‌شود.</li>\n",
    "  <li>نگاشت خطی‌ای می باشد که میانگین مربعات فاصله اقلیدسی بین داده اصلی و داده تبدیل یافته را کمینه می‌کند. </li>\n",
    "    </ul>\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd67a9-aea0-4d11-ac85-5f58269e2356",
   "metadata": {},
   "source": [
    "<div>\n",
    "<center>\n",
    "<img src=\"http://alexhwilliams.info/itsneuronalblog/img/pca/pca_two_views.png\" width=\"500\">\n",
    "     <figcaption><center>شکل3. کمینه‌سازی خطای مربعات در مقایسه با بیشینه‌سازی واریانس</center></figcaption> </td>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e77070-39ad-4805-bcdd-d2074c89f3b2",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    ما در این درسنامه به بررسی تعریف اول یعنی بیشینه سازی واریانس می پردازیم و بررسی درستی تعریف دوم را به خواننده واگذار می کنیم.\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339da8d-2ac7-43aa-8109-cdeaf40ade04",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">1.2. بررسی تعریف بیشینه سازی واریانس در تحلیل مولفه های اصلی</span>\n",
    "    <br>\n",
    "    ابتدا فرص کنید که مجموعه دادگان\n",
    "    $ \\mathcal{D} = \\{ \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{N}\\} $\n",
    "    به ما داده شده است. هر یک از\n",
    "    $ \\mathbf{x}_{i} $\n",
    "    یک بردار داده\n",
    "    $ D $\n",
    "     بعدی می‌باشد. حال ما این بردار‌ها را در درون یک ماتریس قرار می‌دهیم تا ماتریس دادگان یا \n",
    "    <b>Design Matrix</b>\n",
    "    $ \\mathbf{X}_{N \\times D} $\n",
    "    را تشکیل دهیم.\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\\mathbf{X}_{N \\times D}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}^\\top_{1} \\\\\n",
    "\\mathbf{x}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}^\\top_n \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    " x_{1}^{[1]} & x_{2}^{[1]} & \\cdots & x_{D}^{[1]} \\\\\n",
    " x_{1}^{[2]} & x_{2}^{[2]} & \\cdots & x_{D}^{[2]} \\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    " x_{1}^{[N]} & x_{2}^{[N]} & \\cdots & x_{D}^{[N]} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br>\n",
    "    ابتدا به بررسی میانگین نمونه‌ای و کوواریانس نمونه‌ای این ماتریس دادگان می‌پردازیم و سپس زیر فضای اصلی را میابیم \n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac21f2-1e28-4dd6-b697-94b43704a873",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">2.2. بررسی تعریف میانگین نمونه‌ای و واریانس نمونه‌ای ماتریس دادگان</span>\n",
    "    <br>\n",
    "    میانگین نمونه ای ماتریس دادگان را با\n",
    "    $\\bar{\\mathbf{x}}$\n",
    "    نمایش می دهیم و طبق تعریف خواهیم داشت:\n",
    "    <br>\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    \\bar{\\mathbf{x}}_{D \\times 1} = \\frac{1}{N}\\mathbf{X}^\\top \\mathbf{1}\\quad, \\quad \\mathbf{1}_{N \\times 1} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    <br>\n",
    "    ‌برای ماتریس کوواریانس نمونه‌ای که آن را با\n",
    "    $\\mathbf{S}$\n",
    "    نشان می‌دهیم نیز خواهیم داشت:\n",
    "    <br>\n",
    "     $$\n",
    "    \\mathbf{S}_{D \\times D} = \\frac{1}{N - 1}(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top})^\\top(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}) = \\frac{1}{N - 1}\\displaystyle\\sum_{i = 1}^{N}(\\mathbf{x}_{i} - \\bar{\\mathbf{x}})(\\mathbf{x}_{i} - \\bar{\\mathbf{x}})^\\top\n",
    "    $$\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a806c-0d35-4945-ae77-ba0ee73af0b2",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    چند نکته در مورد ماتریس کوواریانس نمونه‌ای\n",
    "    $ \\mathbf{S} $:\n",
    "    <ul>\n",
    "        <li>این ماتریس به وضوح یک ماتریس\n",
    "            <b> متقارن</b>\n",
    "        می‌باشد زیرا داریم:\n",
    "        $ \\mathbf{S}^{\\top} = \\mathbf{S} $\n",
    "        </li>\n",
    "        <li>\n",
    "            طبق قضیه‌ای در جبر خطی، هر ماتریس متقارن\n",
    "            ،$ D \\times D $،\n",
    "            $D$\n",
    "            مقدار ویژه حقیقی به شکل\n",
    "            $ \\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{D} $\n",
    "            دارد که متناظر آن‌ها بردار ویژه‌ای وجود دارد به شکل\n",
    "            $\\{ \\mathbf{q}_{1}, \\mathbf{q}_{1}, \\dots, \\mathbf{q}_{D} \\}$\n",
    "که \n",
    "            <b>متعامد نرمال</b>\n",
    "             می‌باشند. نکته‌ای که وجود دارد آن است که بردارهای ویژه متعامد هستند اما لزوما نرم واحد ندارند. ما می‌توانیم آن را نرمال کنیم و با قرار دادن آن‌ها در ستون های ماتریس\n",
    "            $ \\mathbf{Q} $\n",
    "            ، بردار‌های ویژه متعامد نرمال را به دست بیاوریم.\n",
    "        </li>\n",
    "        <li>\n",
    "            طبق قضیه ای در جبر خطی، هر ماتریس متقارن یک ماتریس قطری شدنی است یعنی چنین تجزیه ای برای آن وجود دارد:\n",
    "            <br>\n",
    "            <br>\n",
    "            $$\\begin{split}\n",
    "            \\mathbf{S} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\top} = \\displaystyle\\sum_{i = 1}^{D} \\lambda_{i}\\mathbf{q}_{i}\\mathbf{q}_{i}^{\\top}, \\quad \\mathbf{\\Lambda} = diag(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{D}), \\quad \\mathbf{Q}= \\begin{bmatrix}\n",
    "\\mathbf{q}_{1} & \\mathbf{q}_{1} & \\dots & \\mathbf{q}_{D}\n",
    "\\end{bmatrix}\n",
    "            \\end{split}\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f5171-29fc-4360-8894-4b26044fd508",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">4.2. بررسی زیرفضای اصلی دادگان تبدیل یافته</span>\n",
    "    <br>\n",
    "    ابتدا فرض می‌کنیم که می‌خواهیم بعد داده‌ها را به مقدار\n",
    "    $M$\n",
    "    به گونه ای که\n",
    "    $M < D$\n",
    "    می‌باشد؛ کاهش دهیم. این فضای برداری را\n",
    "    $\\mathbf{W}$\n",
    "    می‌نامیم و داریم\n",
    "    $ \\mathbf{W} \\subseteq \\mathbb{R}^{D}$.\n",
    "    برای این کار ابتدا فرض می‌کنیم که پایه‌های متعامد نرمال تشکیل دهنده این فضا به شکل \n",
    "    $\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{M}$\n",
    "    می‌باشند. هر کدام از این پایه‌های برداری\n",
    "    $\\mathbf{u}_{i}$،\n",
    "    $D \\times 1$\n",
    "    ‌بوده که داده‌‌ای چون \n",
    "    $\\mathbf{x}$\n",
    "    را به فضای مقصد می‌نگارند. اگر فرض کنیم که با قرار دادن این پایه‌ها در درون ستون‌های یک ماتریس چون \n",
    "    $ \\mathbf{U} $\n",
    "    ماتریس نگاشت به دست می‌آید؛ در اینصورت بردار داده در فضای تبدیل یافته \n",
    "    $ M $\n",
    "    بعدی را با \n",
    "    $\\mathbf{x}^{*}$\n",
    "    نشان می‌دهیم و خواهیم داشت:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\\begin{split}\n",
    "    \\mathbf{x}^{*} = \\mathbf{U}^{\\top}\\mathbf{x}, \\quad \\mathbf{U} = \\begin{bmatrix}\n",
    "\\mathbf{u}_{1} & \\mathbf{u}_{2} & \\dots & \\mathbf{u}_{M}\n",
    "\\end{bmatrix}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    <br>\n",
    "    حال اگر فرض کنیم که می‌خواهیم این موضوع را در مورد کل\n",
    "    $ N $\n",
    "    داده ماتریس دادگان تعمیم دهیم آن را با \n",
    "    $ \\mathbf{X}^{*} $\n",
    "   نشان می‌دهیم و خواهیم داشت: (در عبارت زیر منظور از \n",
    "    $ \\mathbf{X} $\n",
    "    همان \n",
    "    <b>Design Matrix</b>\n",
    "    می‌باشد.)\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    \\mathbf{X}^{*} = \\mathbf{X}\\mathbf{U} $$\n",
    "    حال از آنجایی که نیاز است تا واریانس داده‌ها در هر یک از \n",
    "    $M$\n",
    "     بعد بیشینه شود. باید ابتدا بردار میانگین داده‌های تبدیل یافته و همچنین ماتریس کوواریانس آن‌ها را به دست بیاوریم \n",
    "    این مقادیر را به ترتیب با \n",
    "    $ \\bar{\\mathbf{x}}^{*} $\n",
    "    و\n",
    "    $ \\mathbf{S}^{*} $\n",
    "    نشان می‌دهیم و طبق تعریفی که قبلا ارائه دادیم؛ داریم:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$ \n",
    "    \\bar{\\mathbf{x}}^{*} = \\frac{1}{N} (\\mathbf{X}^{*})^{\\top} \\mathbf{1} = \\frac{1}{N} (\\mathbf{X}\\mathbf{U})^{\\top} \\mathbf{1} = \\frac{1}{N} (\\mathbf{U}^{\\top}\\mathbf{X}^{\\top}) \\mathbf{1} =  \\mathbf{U}^{\\top}(\\frac{1}{N}\\mathbf{X}^{\\top} \\mathbf{1}) = \\mathbf{U}^{\\top}\\bar{\\mathbf{x}} $$\n",
    "    <br>\n",
    "    $$\n",
    "    \\mathbf{S}^{*} =\n",
    "    \\frac{1}{N - 1}\\big(\\mathbf{X}^{*} - \\mathbf{1}(\\bar{\\mathbf{x}}^{*})^{\\top}\\big)^\\top\\big(\\mathbf{X}^{*} - \\mathbf{1}(\\bar{\\mathbf{x}}^{*})^{\\top}\\big) =\n",
    "    \\frac{1}{N - 1}\\big((\\mathbf{X}\\mathbf{U}) - \\mathbf{1}(\\mathbf{U}^{\\top}\\bar{\\mathbf{x}})^{\\top}\\big)^\\top\\big((\\mathbf{X}\\mathbf{U}) - \\mathbf{1}(\\mathbf{U}^{\\top}\\bar{\\mathbf{x}})^{\\top}\\big) = \n",
    "    \\frac{1}{N - 1}\\big((\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top})\\mathbf{U}\\big)^\\top\\big((\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top})\\mathbf{U}\\big) = \n",
    "    \\frac{1}{N - 1}\\mathbf{U}^{\\top}\\big(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}\\big)^\\top\\big(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}\\big)\\mathbf{U} = \n",
    "    \\mathbf{U}^{\\top}\\bigg(\\frac{1}{N - 1}\\big(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}\\big)^\\top\\big(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}\\big)\\bigg)\\mathbf{U} = \\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U}\n",
    "    $$\n",
    "    <br>\n",
    "ما می‌دانیم که واریانس هر بعد (می توان هر بعد را به نوعی محور اعداد حقیقی فرض کرد که واریانس نمونه‌ای \n",
    "    $ S_{i} $\n",
    "    برای آن تعریف می‌شود)\n",
    "    بر روی درایه‌های قطر اصلی ماتریس\n",
    "        $ \\mathbf{S}^{*} $\n",
    "    واقع شده‌است. به منظور بیشینه‌سازی هر واریانس نمونه‌ای، حاصل جمع آن‌ها را بیشینه می‌کنیم:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\\DeclareMathOperator{\\Tr}{Tr} \\max_{S_1, S_2, \\dots, S_{M}} S_1 + S_2 + \\dots + S_{M} \\equiv  \\max_{S_1, S_2, \\dots, S_{M}} \\Tr(\\mathbf{S^{*}}) \\equiv \\max_{S_1, S_2, \\dots, S_{M}} \\Tr(\\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U})$$\n",
    "    <br>\n",
    "    چون حاصل جمع تابعی خطی از تک تک واریانس‌ها می‌باشد؛ بیشینه کردن آن نسبت به واریانس‌ها، بیشینه کردن هر یک از واریانس‌ها را تضمین می‌کند. ما می‌دانیم که \n",
    "    واریانس هر بعد چون \n",
    "    $ S_{i} $\n",
    "    از مولفه‌های تبدیل یافته هر داده توسط پایه متناظر آن بعد یعنی\n",
    "    $ \\mathbf{u}_{i} $\n",
    "    بدست می‌آید\n",
    "    پس در واقع\n",
    "    $S_{i} = \\mathbf{u}_{i}^{\\top} \\mathbf{S} \\mathbf{u}_{i} $\n",
    "    و از آنجایی که ماتریس \n",
    "    $ \\mathbf{S}$\n",
    "    از روی داده‌ها حساب شده است پس متغیر‌های برداری \n",
    "    $ \\mathbf{u}_{i} $\n",
    "    در مسئله بهینه‌سازی استفاده شده و مقدار حاصل جمع باید نسبت به آن‌ها بهینه شود. چون این بردارها متعامد نرمال هستند پس داریم\n",
    "    $\\mathbf{U}^{\\top} \\mathbf{U} = \\mathbf{I}$\n",
    "    و مسئله بهینه‌سازی به شکل زیر خواهد شد:\n",
    "    </div></span>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6de1c-1f9c-4272-920e-9275a6173a78",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <br>\n",
    "    $$\\DeclareMathOperator{\\Tr}{Tr} \\begin{aligned}\n",
    "\\max_{\\mathbf{U}} \\quad & \\Tr(\\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U})\\\\\n",
    "\\textrm{subject to} \\quad & \\mathbf{U} \\in \\mathbb{R}^{D \\times M},\\\\\n",
    "  & \\mathbf{U}^{\\top}\\mathbf{U} = \\mathbf{I}\\\\\n",
    "\\end{aligned}$$\n",
    "    <br>\n",
    "     ثابت می کنیم که\n",
    "    اگر \n",
    "    $\\lambda_{1} \\geq \\lambda_{2} \\geq \\dots \\geq \\lambda_{D}$\n",
    "    مقادیر ویژه ماتریس\n",
    "    $ \\mathbf{S} $\n",
    "    باشند؛ آنگاه بیشینه عبارت بالا برابر\n",
    "    $ \\lambda_{1} + \\lambda_{2} + \\dots + \\lambda_{M} $\n",
    "    می باشد یا به عبارتی حاصل جمع \n",
    "    $M$\n",
    "    مقدار ویژه بزرگ \n",
    "    ماتریس\n",
    "    $ S $\n",
    "    می شود.\n",
    "    در واقع این مقادیر ویژه همان بیشینه مقدار واریانس در هر یک از \n",
    "    $ M $\n",
    "    بعد می‌باشد\n",
    "    $S_{1}^{max} = \\lambda_{1}, S_{2}^{max} = \\lambda_{2}, \\dots, S_{M}^{max} = \\lambda_{M} $\n",
    "    و این زمانی به دست می آید که \n",
    "    ماتریس\n",
    "    $\\mathbf{U}$\n",
    "    ماتریس نگاشت متعامد و شامل بردارهای ویژه \n",
    "    $\\mathbf{U} = \\begin{bmatrix} \\mathbf{q_{1}} & \\mathbf{q_{2}} & \\cdots & \\mathbf{q_{M}} \\end{bmatrix}$\n",
    "    نظیر مقادیر ویژه\n",
    "    $ \\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{M} $\n",
    "    باشد.\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fde405-e67c-483d-94ee-1a2067601a18",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    اثبات:\n",
    "    <br>\n",
    "    ابتدا فرض می کنیم که\n",
    "    $ \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{M} $\n",
    "    بردار های ستونی، ستون های ماتریس\n",
    "    $ U $\n",
    "     می باشند. از آنجایی که گفتیم برای هر ماتریس متقارن یک تجزیه به شکل\n",
    "    $ \\sum_{j = 1}^{D} \\lambda_{j}\\mathbf{q}_{j}\\mathbf{q}_{j}^{\\top} $\n",
    "    وجود دارد در نتیجه عبارت\n",
    "    $\\DeclareMathOperator{\\Tr}{Tr} \\Tr(\\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U})$\n",
    "    به شکل زیر می شود:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    \\DeclareMathOperator{\\Tr}{Tr} \\Tr(\\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U}) = \\displaystyle\\sum_{i = 1}^{M} \\mathbf{u}_{i}^{\\top}\\mathbf{S}\\mathbf{u}_{i} =\n",
    "    \\displaystyle\\sum_{i = 1}^{M} \\mathbf{u}_{i}^{\\top}\\Big(\\displaystyle\\sum_{j = 1}^{D} \\lambda_{j}\\mathbf{q}_{j}\\mathbf{q}_{j}^{\\top}\\Big)\\mathbf{u}_{i} = \n",
    "    \\displaystyle\\sum_{i = 1}^{M}\\displaystyle\\sum_{j = 1}^{D} \\lambda_{j} \\mathbf{u}_{i}^{\\top}\\mathbf{q}_{j}\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i} = \n",
    "    \\displaystyle\\sum_{i = 1}^{M}\\displaystyle\\sum_{j = 1}^{D} \\lambda_{j} \\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} =\n",
    "    \\displaystyle\\sum_{j = 1}^{D} \\lambda_{j}\\displaystyle\\sum_{i = 1}^{M} \\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} =\n",
    "    \\displaystyle\\sum_{j = 1}^{D} \\lambda_{j}c_{j}\n",
    "    $$\n",
    "    <br>\n",
    "    در عبارت بالا مقدار\n",
    "    $c_{j}$\n",
    "    برابر\n",
    "    $c_{j} = \\sum_{i = 1}^{M} \\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2}$\n",
    "    می باشد. ابتدا ثابت می کنیم که مقدار \n",
    "    $c_{j}$\n",
    "    بین 0 و 1 می باشد یعنی \n",
    "    $0 \\leq c_{j} \\leq 1$\n",
    "    و ثانیا ثابت می کنیم که \n",
    "    $\\sum_{j = 1}^{D} c_{j} = M$.\n",
    "    <br>\n",
    "    می دانیم که به وضوح\n",
    "    $ c_{j} \\geq 0 $\n",
    "    برقرار است زیرا مجموع مقادیر نامنفی می‌باشد\n",
    "    حال برای اثبات \n",
    "    $ c_{j} \\leq 1 $\n",
    "    مقادیر برداری \n",
    "    $\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots,  \\mathbf{u}_{M}$\n",
    "    را به پایه‌های متعامد نرمال\n",
    "    $\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots,  \\mathbf{u}_{D}$\n",
    "    فضای برداری\n",
    "    $\\mathbb{R}^{D} $\n",
    "    گسترده می‌کنیم و داریم:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    c_{j} = \\displaystyle\\sum_{i = 1}^{M}\\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} \\leq \\displaystyle\\sum_{i = 1}^{D}\\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} = \n",
    "    \\| \\mathbf{q}_{j}^{\\top} \\mathbf{U} \\|_{2}^{2} =\n",
    "    (\\mathbf{q}_{j}^{\\top} \\mathbf{U})(\\mathbf{q}_{j}^{\\top} \\mathbf{U})^{\\top}\n",
    "    = \\| \\mathbf{q}_{j} \\|_{2}^{2} = 1\n",
    "    $$\n",
    "    <br>\n",
    "    و چون \n",
    "    $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,  \\mathbf{q}_{D}$\n",
    "    پایه‌های متعامد نرمالی برای فضای برداری \n",
    "    $\\mathbb{R}^{D} $\n",
    "    هستند؛ نیز داریم:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\\displaystyle\\sum_{j = 1}^{D} c_{j} = \\displaystyle\\sum_{j = 1}^{D}\\displaystyle\\sum_{i = 1}^{M}\\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} =\n",
    "    \\displaystyle\\sum_{i = 1}^{M}\\displaystyle\\sum_{j = 1}^{D}\\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} =\n",
    "    \\displaystyle\\sum_{i = 1}^{M}\\| \\mathbf{u}_{i} \\|_{2}^{2} =\n",
    "    \\displaystyle\\sum_{i = 1}^{M}1 = M$$\n",
    "    <br> \n",
    "     بنابراین \n",
    "    عبارت \n",
    "    $ \\sum_{j = 1}^{D} \\lambda_{j}c_{j}$\n",
    "    زمانی بیشینه میشود که با داده شدن مقادیر ویژه، هر یک از ضرایب آن‌ها بیشینه شود و مقدار 1 را به خود بگیرد از طرفی چون رابطه تساوی\n",
    "    $ \\sum_{j = 1}^{D} c_{j} = M$\n",
    "    باید برقرار باشد خواهیم داشت:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    c_{1} = c_{2} =  \\dots = c_{M} = 1, \\quad c_{M+1} = c_{M+2} =  \\dots = c_{D} = 0\n",
    "    \\end{split}\n",
    "    $$\n",
    "    <br>\n",
    "    در نتیجه تساوی \n",
    "    $Span[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,  \\mathbf{q}_{M}] = Span[\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots,  \\mathbf{u}_{M}]$\n",
    "    باید برقرار باشد \n",
    "    و در نتیجه مقادیر برداری \n",
    "    $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,  \\mathbf{q}_{M}$\n",
    "    پایه‌هایی برای فضای مقصد می‌باشند که در ستون‌های ماتریس \n",
    "    $\\mathbf{U}$\n",
    "    قرار گرفته‌اند.\n",
    "    <br>\n",
    "    <br>\n",
    "    خلاصه و جمع‌بندی:\n",
    "    <br>\n",
    "     برای پیداکردن مجموعه دادگان تبدیل یافته با بعد کمتر به کمک روش تحلیل مولفه‌های اصلی باید مراحل زیر را طی کنیم. فرض می‌کنیم که مجموعه دادگان\n",
    "    $ \\mathbf{X}_{N \\times D}$\n",
    "    که \n",
    "    $D$\n",
    "    بعدی است داده شده و می‌خواهیم آن را به \n",
    "    $\\mathbf{X}^{*}_{N \\times M}$\n",
    "    تبدیل کنیم که \n",
    "    $ M $\n",
    "    بعدی است:\n",
    "    <ol>\n",
    "  <li>ابتدا ماتریس کوواریانس نمونه‌ای \n",
    "        $ S $\n",
    "      را طبق فرمول \n",
    "      $\\mathbf{S}_{D \\times D} = \\frac{1}{N - 1}(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top})^\\top(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}) $\n",
    "        بدست می‌آوریم که در آن\n",
    "      $\\bar{\\mathbf{x}}_{D \\times 1} = \\frac{1}{N}\\mathbf{X}^\\top \\mathbf{1}$\n",
    "        </li>\n",
    "  <li>مقادیر ویژه\n",
    "      $\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{M}$\n",
    "       و بردار‌های ویژه نظیر آن‌ها \n",
    "        $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,  \\mathbf{q}_{M}$\n",
    "       را برای ماتریس\n",
    "      $ S $\n",
    "       می‌یابیم\n",
    "        </li>\n",
    "  <li>برای کاهش بعد داده‌ها به \n",
    "        $M$\n",
    "        بعد، ابتدا این مقادیر ویژه را به شکل نزولی مرتب می‌کنیم\n",
    "        $\\lambda_{1} \\geq \\lambda_{2} \\geq \\dots \\geq \\lambda_{D}$\n",
    "        سپس \n",
    "        $M$\n",
    "        تا از بزرگترین آن‌ها را به شکل \n",
    "        $\\lambda_{1} \\geq \\lambda_{2} \\geq \\dots \\geq \\lambda_{M}$\n",
    "        بر می‌گزینیم و بردار‌های ویژه نظیر آن‌ها را انتخاب می‌کنیم\n",
    "        $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,\\mathbf{q}_{M} $\n",
    "        </li>\n",
    "        <li>\n",
    "        مقادیر ویژه \n",
    "        $\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{M}$\n",
    "        به ترتیب واریانس نمونه‌ای دادگان تبدیل یافته در هر یک از \n",
    "            $ M $\n",
    "            بعد جدید خواهند بود و با قرار دادن بردار‌های ویژه \n",
    "            $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,\\mathbf{q}_{M}$\n",
    "            در ستون‌های ماتریس \n",
    "            $ \\mathbf{U}_{D \\times M} $\n",
    "            به شکل \n",
    "            $\\mathbf{U}_{D \\times M} = \\begin{bmatrix} \\mathbf{q}_{1}& \\mathbf{q}_{2}& \\cdots&\\mathbf{q}_{M}\\end{bmatrix}$\n",
    "            ؛ ماتریس نگاشت را می‌سازیم.\n",
    "        </li>\n",
    "        <li>\n",
    "          در نهایت با اعمال نگاشت به ماتریس دادگان طبق رابطه\n",
    "        $\\mathbf{X}^{*} = \\mathbf{X}\\mathbf{U}$\n",
    "        ماتریس \n",
    "            $\\mathbf{X}^{*}$\n",
    "        را بدست می‌اوریم.\n",
    "        </li>\n",
    "    </ol>\n",
    "    </div></span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d8ee5-66f8-4c74-a765-b418923e6089",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \"> 5.2. بررسی مجموعه دادگان  Irirs </span>\n",
    "    <br>\n",
    "    برای مشاهده عملکرد PCA به منظور کاهش بعد داده‌ها ، مجموعه دادگان Iris را در نظر می‌گیریم. از آنجایی که داده‌های این مجموعه 4 بعدی هستند؛ قصد داریم تا آن‌ها را به 2 و 3 بعد کاهش دهیم.\n",
    "    </div></span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea044b-5de8-47ac-a348-928daacf2e7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
